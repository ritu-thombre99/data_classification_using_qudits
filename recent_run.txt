python run_qubit.py --encoding_and_rotation B --dataset_size 500
Initial parameters: [ 0.92102497  1.03530095 -1.8184938   0.30931402]
Iteration: 1 / 220 Loss: 0.7172023125638636
Iteration: 2 / 220 Loss: 0.7009556276692153
Iteration: 3 / 220 Loss: 0.6826346606682171
Iteration: 4 / 220 Loss: 0.6733482178552944
Iteration: 5 / 220 Loss: 0.6550444314292647
Iteration: 6 / 220 Loss: 0.6437596410551272
Iteration: 7 / 220 Loss: 0.6279903030565734
Iteration: 8 / 220 Loss: 0.6190106123138626
Iteration: 9 / 220 Loss: 0.6102620174709827
Iteration: 10 / 220 Loss: 0.6014408321800415
Iteration: 11 / 220 Loss: 0.5860921370064639
Iteration: 12 / 220 Loss: 0.5754753373251759
Iteration: 13 / 220 Loss: 0.5649519700219466
Iteration: 14 / 220 Loss: 0.5524735573897364
Iteration: 15 / 220 Loss: 0.5448129886463501
Iteration: 16 / 220 Loss: 0.5373152195343657
Iteration: 17 / 220 Loss: 0.5255189563060532
Iteration: 18 / 220 Loss: 0.5161182199326787
Iteration: 19 / 220 Loss: 0.5003042994444312
Iteration: 20 / 220 Loss: 0.4893277940104854
Iteration: 21 / 220 Loss: 0.47200127331825803
Iteration: 22 / 220 Loss: 0.4619239279988762
Iteration: 23 / 220 Loss: 0.4542978682819969
Iteration: 24 / 220 Loss: 0.44455762313388475
Iteration: 25 / 220 Loss: 0.4329209173195214
Iteration: 26 / 220 Loss: 0.42369578179011613
Iteration: 27 / 220 Loss: 0.4123787822222751
Iteration: 28 / 220 Loss: 0.40352867407969195
Iteration: 29 / 220 Loss: 0.3970272883502653
Iteration: 30 / 220 Loss: 0.3929531794150707
Iteration: 31 / 220 Loss: 0.38899453937306117
Iteration: 32 / 220 Loss: 0.3806133954517802
Iteration: 33 / 220 Loss: 0.37467602746175477
Iteration: 34 / 220 Loss: 0.3687944949187765
Iteration: 35 / 220 Loss: 0.36081956279252764
Iteration: 36 / 220 Loss: 0.36188648795455813
Iteration: 37 / 220 Loss: 0.35640095734489863
Iteration: 38 / 220 Loss: 0.3509960285587601
Iteration: 39 / 220 Loss: 0.3478941068817255
Iteration: 40 / 220 Loss: 0.34038971161216874
Iteration: 41 / 220 Loss: 0.32206369579314076
Iteration: 42 / 220 Loss: 0.31749820790450106
Iteration: 43 / 220 Loss: 0.31973444863392414
Iteration: 44 / 220 Loss: 0.31538042001224653
Iteration: 45 / 220 Loss: 0.311058046029057
Iteration: 46 / 220 Loss: 0.3067501948134069
Iteration: 47 / 220 Loss: 0.30247142352075584
Iteration: 48 / 220 Loss: 0.29601727717440307
Iteration: 49 / 220 Loss: 0.29191196934136643
Iteration: 50 / 220 Loss: 0.2856802749810936
Iteration: 51 / 220 Loss: 0.2839737361274424
Iteration: 52 / 220 Loss: 0.28227283387690977
Iteration: 53 / 220 Loss: 0.280601859515526
Iteration: 54 / 220 Loss: 0.27678246052696337
Iteration: 55 / 220 Loss: 0.2730077081635257
Iteration: 56 / 220 Loss: 0.27373172398142914
Iteration: 57 / 220 Loss: 0.2656413385002149
Iteration: 58 / 220 Loss: 0.2621090033404733
Iteration: 59 / 220 Loss: 0.2564147288605559
Iteration: 60 / 220 Loss: 0.25523264897430975
Iteration: 61 / 220 Loss: 0.2518331250990374
Iteration: 62 / 220 Loss: 0.2506928220589393
Iteration: 63 / 220 Loss: 0.24959635641738404
Iteration: 64 / 220 Loss: 0.2485040315790307
Iteration: 65 / 220 Loss: 0.24520531087857514
Iteration: 66 / 220 Loss: 0.24417366105582158
Iteration: 67 / 220 Loss: 0.24092456063760706
Iteration: 68 / 220 Loss: 0.23768480660005384
Iteration: 69 / 220 Loss: 0.23445561941218096
Iteration: 70 / 220 Loss: 0.2357109325448429
Iteration: 71 / 220 Loss: 0.2347498483580912
Iteration: 72 / 220 Loss: 0.23157396000882854
Iteration: 73 / 220 Loss: 0.23063458290561065
Iteration: 74 / 220 Loss: 0.23192535985141619
Iteration: 75 / 220 Loss: 0.22879191937608095
Iteration: 76 / 220 Loss: 0.2257086364210299
Iteration: 77 / 220 Loss: 0.22486143556622526
Iteration: 78 / 220 Loss: 0.21957907952701325
Iteration: 79 / 220 Loss: 0.21877381955831976
Iteration: 80 / 220 Loss: 0.2157537063699243
Iteration: 81 / 220 Loss: 0.21278037937556452
Iteration: 82 / 220 Loss: 0.2120397019032085
Iteration: 83 / 220 Loss: 0.21130153578323327
Iteration: 84 / 220 Loss: 0.210565918004851
Iteration: 85 / 220 Loss: 0.2120608304754248
Iteration: 86 / 220 Loss: 0.21134115637048026
Iteration: 87 / 220 Loss: 0.2084272673078961
Iteration: 88 / 220 Loss: 0.20779335964968945
Iteration: 89 / 220 Loss: 0.20494586642512447
Iteration: 90 / 220 Loss: 0.19994087223064377
Iteration: 91 / 220 Loss: 0.19941554351616803
Iteration: 92 / 220 Loss: 0.19446134992350791
Iteration: 93 / 220 Loss: 0.193973120944596
Iteration: 94 / 220 Loss: 0.19127674022455815
Iteration: 95 / 220 Loss: 0.19083142130639566
Iteration: 96 / 220 Loss: 0.1881658328677791
Iteration: 97 / 220 Loss: 0.1833105376622001
Iteration: 98 / 220 Loss: 0.18293644082305224
Iteration: 99 / 220 Loss: 0.1825629897358459
Iteration: 100 / 220 Loss: 0.18219019357934596
Iteration: 101 / 220 Loss: 0.17960533297117495
Iteration: 102 / 220 Loss: 0.1792612333068371
Iteration: 103 / 220 Loss: 0.178917624138408
Iteration: 104 / 220 Loss: 0.17857451438960606
Iteration: 105 / 220 Loss: 0.17823191290861315
Iteration: 106 / 220 Loss: 0.17566762158847346
Iteration: 107 / 220 Loss: 0.17533233449443142
Iteration: 108 / 220 Loss: 0.17278199166181868
Iteration: 109 / 220 Loss: 0.170232765829834
Iteration: 110 / 220 Loss: 0.1676940248294794
Iteration: 111 / 220 Loss: 0.169608185546253
Iteration: 112 / 220 Loss: 0.1693016677469161
Iteration: 113 / 220 Loss: 0.1689955528063081
Iteration: 114 / 220 Loss: 0.16868984855806532
Iteration: 115 / 220 Loss: 0.16838456276935201
Iteration: 116 / 220 Loss: 0.16807970314018914
Iteration: 117 / 220 Loss: 0.16556906003434477
Iteration: 118 / 220 Loss: 0.16529639453164352
Iteration: 119 / 220 Loss: 0.16502399706137102
Iteration: 120 / 220 Loss: 0.1647518744046435
Iteration: 121 / 220 Loss: 0.16448003329703423
Iteration: 122 / 220 Loss: 0.16420848042790706
Iteration: 123 / 220 Loss: 0.1639372224397625
Iteration: 124 / 220 Loss: 0.16366626592759734
Iteration: 125 / 220 Loss: 0.16784313528660808
Iteration: 126 / 220 Loss: 0.16536001351682045
Iteration: 127 / 220 Loss: 0.16509983356250038
Iteration: 128 / 220 Loss: 0.16483997923928223
Iteration: 129 / 220 Loss: 0.16680790060266068
Iteration: 130 / 220 Loss: 0.16213023487355396
Iteration: 131 / 220 Loss: 0.15970356072226682
Iteration: 132 / 220 Loss: 0.15729719911280274
Iteration: 133 / 220 Loss: 0.15712600490758077
Iteration: 134 / 220 Loss: 0.15695494440518923
Iteration: 135 / 220 Loss: 0.15678402002133435
Iteration: 136 / 220 Loss: 0.1566132341616319
Iteration: 137 / 220 Loss: 0.15422195974286396
Iteration: 138 / 220 Loss: 0.1540543441185538
Iteration: 139 / 220 Loss: 0.15388689345653192
Iteration: 140 / 220 Loss: 0.1537196096979612
Iteration: 141 / 220 Loss: 0.15355249477642524
Iteration: 142 / 220 Loss: 0.15338555061776468
Iteration: 143 / 220 Loss: 0.15321877913991466
Iteration: 144 / 220 Loss: 0.1530521822527442
Iteration: 145 / 220 Loss: 0.15288576185789637
Iteration: 146 / 220 Loss: 0.15271951984863086
Iteration: 147 / 220 Loss: 0.15255345810966725
Iteration: 148 / 220 Loss: 0.1523875785170308
Iteration: 149 / 220 Loss: 0.1522218829378988
Iteration: 150 / 220 Loss: 0.15427962419414648
Iteration: 151 / 220 Loss: 0.15411560917466552
Iteration: 152 / 220 Loss: 0.15173017607271086
Iteration: 153 / 220 Loss: 0.14935309691119059
Iteration: 154 / 220 Loss: 0.146978453007903
Iteration: 155 / 220 Loss: 0.14683166632661224
Iteration: 156 / 220 Loss: 0.146685012855206
Iteration: 157 / 220 Loss: 0.1420942044208788
Iteration: 158 / 220 Loss: 0.1463938776586237
Iteration: 159 / 220 Loss: 0.14180399702429108
Iteration: 160 / 220 Loss: 0.1438810221291089
Iteration: 161 / 220 Loss: 0.14373604255083314
Iteration: 162 / 220 Loss: 0.1413691330997918
Iteration: 163 / 220 Loss: 0.14566932321939732
Iteration: 164 / 220 Loss: 0.13885871626395835
Iteration: 165 / 220 Loss: 0.13871865632039163
Iteration: 166 / 220 Loss: 0.14080374175346894
Iteration: 167 / 220 Loss: 0.140670685908943
Iteration: 168 / 220 Loss: 0.14498292382467942
Iteration: 169 / 220 Loss: 0.1404060739911111
Iteration: 170 / 220 Loss: 0.1424957280325338
Iteration: 171 / 220 Loss: 0.14014141279718736
Iteration: 172 / 220 Loss: 0.14445436281684235
Iteration: 173 / 220 Loss: 0.13987791290107648
Iteration: 174 / 220 Loss: 0.14196834120258903
Iteration: 175 / 220 Loss: 0.13961447852856354
Iteration: 176 / 220 Loss: 0.14392811152021856
Iteration: 177 / 220 Loss: 0.1393521452701188
Iteration: 178 / 220 Loss: 0.14366546376691247
Iteration: 179 / 220 Loss: 0.13909041452196205
Iteration: 180 / 220 Loss: 0.1411815490272863
Iteration: 181 / 220 Loss: 0.13882877975968208
Iteration: 182 / 220 Loss: 0.1431429249777928
Iteration: 183 / 220 Loss: 0.1385682846818681
Iteration: 184 / 220 Loss: 0.1406601912436973
Iteration: 185 / 220 Loss: 0.14053014571211472
Iteration: 186 / 220 Loss: 0.13595668245565565
Iteration: 187 / 220 Loss: 0.14028718405766752
Iteration: 188 / 220 Loss: 0.13795083958292492
Iteration: 189 / 220 Loss: 0.14005942777523767
Iteration: 190 / 220 Loss: 0.13772385387681077
Iteration: 191 / 220 Loss: 0.13761558089204157
Iteration: 192 / 220 Loss: 0.1352977678009746
Iteration: 193 / 220 Loss: 0.13298076483426446
Iteration: 194 / 220 Loss: 0.13510965103688077
Iteration: 195 / 220 Loss: 0.13724189807096415
Iteration: 196 / 220 Loss: 0.13715458595590813
Iteration: 197 / 220 Loss: 0.13706738563522372
Iteration: 198 / 220 Loss: 0.1369802976714298
Iteration: 199 / 220 Loss: 0.13467574346951589
Iteration: 200 / 220 Loss: 0.1345995259741231
Iteration: 201 / 220 Loss: 0.1345233982072623
Iteration: 202 / 220 Loss: 0.13444736057856405
Iteration: 203 / 220 Loss: 0.13437141349399115
Iteration: 204 / 220 Loss: 0.1365203130215511
Iteration: 205 / 220 Loss: 0.13645433715631142
Iteration: 206 / 220 Loss: 0.13638846379740108
Iteration: 207 / 220 Loss: 0.1363226931064032
Iteration: 208 / 220 Loss: 0.13403514178422687
Iteration: 209 / 220 Loss: 0.13397006821317242
Iteration: 210 / 220 Loss: 0.13390508771902812
Iteration: 211 / 220 Loss: 0.13606250979676768
Iteration: 212 / 220 Loss: 0.13377555177841266
Iteration: 213 / 220 Loss: 0.13593564364534957
Iteration: 214 / 220 Loss: 0.13810393435862095
Iteration: 215 / 220 Loss: 0.13805034992101534
Iteration: 216 / 220 Loss: 0.13577469017139585
Iteration: 217 / 220 Loss: 0.13572673878271269
Iteration: 218 / 220 Loss: 0.13569175807521675
Iteration: 219 / 220 Loss: 0.13566320549986086
Iteration: 220 / 220 Loss: 0.1378572235054165
Training accuracy = 0.8888888888888888
Testing accuracy = 0.94
